{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting detoxify\n",
      "  Using cached detoxify-0.5.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from detoxify) (4.48.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from detoxify) (2.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.94 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from detoxify) (0.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.7.0->detoxify) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.7.0->detoxify) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->detoxify) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.7.0->detoxify) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->detoxify) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->detoxify) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->detoxify) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->detoxify) (2025.1.31)\n",
      "Using cached detoxify-0.5.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: detoxify\n",
      "Successfully installed detoxify-0.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from detoxify import Detoxify\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_toxicity(csv_file_path, save_results=True, sample_size=None):\n",
    "    print(f\"Loading data from {csv_file_path}...\")\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    print(f\"Analyzing all {len(df)} comments...\")\n",
    " \n",
    "    print(\"Loading Detoxify model...\")\n",
    "    model = Detoxify('original')\n",
    "    \n",
    "    # Process each comment\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if i % 500 == 0 and i > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            comments_per_second = i / elapsed\n",
    "            estimated_total = elapsed * (len(df) / i)\n",
    "            print(f\"Processed {i}/{len(df)} comments ({comments_per_second:.2f} comments/sec, estimated total time: {estimated_total/60:.1f} min)\")\n",
    "        \n",
    "        comment = row['comment']\n",
    "        if pd.isna(comment) or comment.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            scores = model.predict(comment)\n",
    "            \n",
    "            # Add original data\n",
    "            for col in df.columns:\n",
    "                scores[col] = row[col]\n",
    "            results.append(scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing comment {i}: {e}\")\n",
    "            print(f\"Comment: {comment}\")\n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    results_df['primary_toxicity_type'] = results_df.apply(get_primary_toxicity, axis=1)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Analysis completed in {elapsed_time:.2f} seconds ({len(results)/elapsed_time:.2f} comments/sec)\")\n",
    "    \n",
    "    if save_results:\n",
    "        toxicity_cols = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']\n",
    "        output_cols = ['user', 'comment'] + ['primary_toxicity_type'] + toxicity_cols \n",
    "        output_file = csv_file_path.replace('.csv', '_with_toxicity.csv')\n",
    "        results_df[output_cols].to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_toxicity(row):\n",
    "    \n",
    "    specific_toxicity_cols = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']\n",
    "    \n",
    "    max_specific_category = max(specific_toxicity_cols, key=lambda col: row[col])\n",
    "    max_specific_score = row[max_specific_category]\n",
    "    \n",
    "    # Only assign a specific toxicity type if the score is above a threshold\n",
    "    threshold = 0.5  \n",
    "    if max_specific_score >= threshold:\n",
    "        return max_specific_category.replace('_', ' ').title()\n",
    "    # If no specific category is high enough but general toxicity is high\n",
    "    elif row['toxicity'] >= threshold:\n",
    "        return \"General Toxicity\"\n",
    "    else:\n",
    "        return \"Non-toxic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity_distribution(df):\n",
    "    if 'primary_toxicity_type' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'primary_toxicity_type' column\")\n",
    "    \n",
    "    toxicity_counts = df['primary_toxicity_type'].value_counts()\n",
    "    \n",
    "    total_comments = len(df)\n",
    "    toxicity_percentages = (toxicity_counts / total_comments * 100).round(2)\n",
    "    \n",
    "    distribution_df = pd.DataFrame({\n",
    "        'count': toxicity_counts,\n",
    "        'percentage': toxicity_percentages\n",
    "    })\n",
    "    \n",
    "    distribution_df = distribution_df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\n===== TOXICITY DISTRIBUTION =====\")\n",
    "    print(f\"Total comments analyzed: {total_comments}\")\n",
    "    print(\"\\nDistribution by toxicity type:\")\n",
    "    \n",
    "    for toxicity_type, row in distribution_df.iterrows():\n",
    "        print(f\"{toxicity_type}: {row['count']} comments ({row['percentage']}%)\")\n",
    "    return distribution_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HasanAbi.csv...\n",
      "Analyzing all 299 comments...\n",
      "Loading Detoxify model...\n",
      "Processed 100/299 comments (43.06 comments/sec, estimated total time: 0.1 min)\n",
      "Processed 200/299 comments (43.45 comments/sec, estimated total time: 0.1 min)\n",
      "Analysis completed in 6.93 seconds (43.14 comments/sec)\n",
      "Results saved to HasanAbi_with_toxicity.csv\n",
      "\n",
      "===== TOXICITY DISTRIBUTION =====\n",
      "Total comments analyzed: 299\n",
      "\n",
      "Distribution by toxicity type:\n",
      "Non-toxic: 261.0 comments (87.29%)\n",
      "General Toxicity: 22.0 comments (7.36%)\n",
      "Obscene: 16.0 comments (5.35%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file_path = \"HasanAbi.csv\"\n",
    "    \n",
    "results = analyze_toxicity(csv_file_path, save_results=True, sample_size=None)\n",
    "\n",
    "toxicity_distribution = get_toxicity_distribution(results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyze_toxicity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHasanAbi-11478.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m analyze_toxicity(csv_file_path, save_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m toxicity_distribution \u001b[38;5;241m=\u001b[39m get_toxicity_distribution(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_toxicity' is not defined"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"HasanAbi-11478.csv\"\n",
    "    \n",
    "results = analyze_toxicity(csv_file_path, save_results=True, sample_size=None)\n",
    "\n",
    "toxicity_distribution = get_toxicity_distribution(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from zizarian.csv...\n",
      "Analyzing all 1744 comments...\n",
      "Loading Detoxify model...\n",
      "Processed 500/1744 comments (46.17 comments/sec, estimated total time: 0.6 min)\n",
      "Processed 1000/1744 comments (45.87 comments/sec, estimated total time: 0.6 min)\n",
      "Processed 1500/1744 comments (45.13 comments/sec, estimated total time: 0.6 min)\n",
      "Analysis completed in 38.99 seconds (44.73 comments/sec)\n",
      "Results saved to zizarian_with_toxicity.csv\n",
      "\n",
      "===== TOXICITY DISTRIBUTION =====\n",
      "Total comments analyzed: 1744\n",
      "\n",
      "Distribution by toxicity type:\n",
      "Non-toxic: 1602.0 comments (91.86%)\n",
      "Obscene: 70.0 comments (4.01%)\n",
      "General Toxicity: 64.0 comments (3.67%)\n",
      "Insult: 6.0 comments (0.34%)\n",
      "Threat: 1.0 comments (0.06%)\n",
      "Identity Attack: 1.0 comments (0.06%)\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"zizarian.csv\"\n",
    "    \n",
    "results = analyze_toxicity(csv_file_path, save_results=True, sample_size=None)\n",
    "\n",
    "toxicity_distribution = get_toxicity_distribution(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from loltyler.csv...\n",
      "Analyzing all 52359 comments...\n",
      "Loading Detoxify model...\n",
      "Processed 500/52359 comments (44.35 comments/sec, estimated total time: 19.7 min)\n",
      "Processed 1000/52359 comments (42.78 comments/sec, estimated total time: 20.4 min)\n",
      "Processed 1500/52359 comments (43.48 comments/sec, estimated total time: 20.1 min)\n",
      "Processed 2000/52359 comments (44.26 comments/sec, estimated total time: 19.7 min)\n",
      "Processed 2500/52359 comments (44.33 comments/sec, estimated total time: 19.7 min)\n",
      "Processed 3000/52359 comments (44.73 comments/sec, estimated total time: 19.5 min)\n",
      "Processed 3500/52359 comments (44.83 comments/sec, estimated total time: 19.5 min)\n",
      "Processed 4000/52359 comments (44.78 comments/sec, estimated total time: 19.5 min)\n",
      "Processed 4500/52359 comments (44.67 comments/sec, estimated total time: 19.5 min)\n",
      "Processed 5000/52359 comments (45.05 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 5500/52359 comments (45.01 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 6000/52359 comments (45.03 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 6500/52359 comments (45.08 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 7000/52359 comments (45.07 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 7500/52359 comments (45.06 comments/sec, estimated total time: 19.4 min)\n",
      "Processed 8000/52359 comments (19.40 comments/sec, estimated total time: 45.0 min)\n",
      "Processed 8500/52359 comments (20.07 comments/sec, estimated total time: 43.5 min)\n",
      "Processed 9000/52359 comments (20.71 comments/sec, estimated total time: 42.1 min)\n",
      "Processed 9500/52359 comments (21.31 comments/sec, estimated total time: 40.9 min)\n",
      "Processed 10000/52359 comments (21.86 comments/sec, estimated total time: 39.9 min)\n",
      "Processed 10500/52359 comments (22.38 comments/sec, estimated total time: 39.0 min)\n",
      "Processed 11000/52359 comments (22.86 comments/sec, estimated total time: 38.2 min)\n",
      "Processed 11500/52359 comments (23.36 comments/sec, estimated total time: 37.4 min)\n",
      "Processed 12000/52359 comments (23.79 comments/sec, estimated total time: 36.7 min)\n",
      "Processed 12500/52359 comments (24.23 comments/sec, estimated total time: 36.0 min)\n",
      "Processed 13000/52359 comments (24.61 comments/sec, estimated total time: 35.5 min)\n",
      "Processed 13500/52359 comments (25.01 comments/sec, estimated total time: 34.9 min)\n",
      "Processed 14000/52359 comments (16.95 comments/sec, estimated total time: 51.5 min)\n",
      "Processed 14500/52359 comments (10.90 comments/sec, estimated total time: 80.0 min)\n",
      "Processed 15000/52359 comments (11.18 comments/sec, estimated total time: 78.1 min)\n",
      "Processed 15500/52359 comments (11.45 comments/sec, estimated total time: 76.2 min)\n",
      "Processed 16000/52359 comments (11.72 comments/sec, estimated total time: 74.5 min)\n",
      "Processed 16500/52359 comments (11.98 comments/sec, estimated total time: 72.8 min)\n",
      "Processed 17000/52359 comments (12.23 comments/sec, estimated total time: 71.3 min)\n",
      "Processed 17500/52359 comments (12.48 comments/sec, estimated total time: 69.9 min)\n",
      "Processed 18000/52359 comments (12.73 comments/sec, estimated total time: 68.5 min)\n",
      "Processed 18500/52359 comments (12.97 comments/sec, estimated total time: 67.3 min)\n",
      "Processed 19000/52359 comments (13.21 comments/sec, estimated total time: 66.1 min)\n",
      "Processed 19500/52359 comments (13.45 comments/sec, estimated total time: 64.9 min)\n",
      "Processed 20000/52359 comments (13.68 comments/sec, estimated total time: 63.8 min)\n",
      "Processed 20500/52359 comments (13.90 comments/sec, estimated total time: 62.8 min)\n",
      "Processed 21000/52359 comments (14.12 comments/sec, estimated total time: 61.8 min)\n",
      "Processed 21500/52359 comments (14.34 comments/sec, estimated total time: 60.9 min)\n",
      "Processed 22000/52359 comments (14.55 comments/sec, estimated total time: 60.0 min)\n",
      "Processed 22500/52359 comments (14.76 comments/sec, estimated total time: 59.1 min)\n",
      "Processed 23000/52359 comments (14.97 comments/sec, estimated total time: 58.3 min)\n",
      "Processed 23500/52359 comments (15.17 comments/sec, estimated total time: 57.5 min)\n",
      "Processed 24000/52359 comments (15.37 comments/sec, estimated total time: 56.8 min)\n",
      "Processed 24500/52359 comments (15.57 comments/sec, estimated total time: 56.0 min)\n",
      "Processed 25000/52359 comments (15.76 comments/sec, estimated total time: 55.4 min)\n",
      "Processed 25500/52359 comments (15.95 comments/sec, estimated total time: 54.7 min)\n",
      "Processed 26000/52359 comments (16.14 comments/sec, estimated total time: 54.1 min)\n",
      "Processed 26500/52359 comments (16.32 comments/sec, estimated total time: 53.5 min)\n",
      "Processed 27000/52359 comments (16.50 comments/sec, estimated total time: 52.9 min)\n",
      "Processed 27500/52359 comments (16.68 comments/sec, estimated total time: 52.3 min)\n",
      "Processed 28000/52359 comments (16.86 comments/sec, estimated total time: 51.7 min)\n",
      "Processed 28500/52359 comments (17.04 comments/sec, estimated total time: 51.2 min)\n",
      "Processed 29000/52359 comments (17.21 comments/sec, estimated total time: 50.7 min)\n",
      "Processed 29500/52359 comments (17.38 comments/sec, estimated total time: 50.2 min)\n",
      "Processed 30000/52359 comments (17.55 comments/sec, estimated total time: 49.7 min)\n",
      "Processed 30500/52359 comments (17.72 comments/sec, estimated total time: 49.2 min)\n",
      "Processed 31000/52359 comments (17.88 comments/sec, estimated total time: 48.8 min)\n",
      "Processed 31500/52359 comments (18.05 comments/sec, estimated total time: 48.4 min)\n",
      "Processed 32000/52359 comments (18.21 comments/sec, estimated total time: 47.9 min)\n",
      "Processed 32500/52359 comments (18.36 comments/sec, estimated total time: 47.5 min)\n",
      "Processed 33000/52359 comments (18.51 comments/sec, estimated total time: 47.1 min)\n",
      "Processed 33500/52359 comments (18.67 comments/sec, estimated total time: 46.8 min)\n",
      "Processed 34000/52359 comments (18.82 comments/sec, estimated total time: 46.4 min)\n",
      "Processed 34500/52359 comments (18.96 comments/sec, estimated total time: 46.0 min)\n",
      "Processed 35000/52359 comments (19.10 comments/sec, estimated total time: 45.7 min)\n",
      "Processed 35500/52359 comments (19.24 comments/sec, estimated total time: 45.4 min)\n",
      "Processed 36000/52359 comments (19.38 comments/sec, estimated total time: 45.0 min)\n",
      "Processed 36500/52359 comments (19.51 comments/sec, estimated total time: 44.7 min)\n",
      "Processed 37000/52359 comments (19.65 comments/sec, estimated total time: 44.4 min)\n",
      "Processed 37500/52359 comments (19.79 comments/sec, estimated total time: 44.1 min)\n",
      "Processed 38000/52359 comments (19.92 comments/sec, estimated total time: 43.8 min)\n",
      "Processed 38500/52359 comments (20.06 comments/sec, estimated total time: 43.5 min)\n",
      "Processed 39000/52359 comments (20.18 comments/sec, estimated total time: 43.2 min)\n",
      "Processed 39500/52359 comments (20.31 comments/sec, estimated total time: 43.0 min)\n",
      "Processed 40000/52359 comments (20.44 comments/sec, estimated total time: 42.7 min)\n",
      "Processed 40500/52359 comments (20.56 comments/sec, estimated total time: 42.4 min)\n",
      "Processed 41000/52359 comments (20.68 comments/sec, estimated total time: 42.2 min)\n",
      "Processed 41500/52359 comments (20.80 comments/sec, estimated total time: 41.9 min)\n",
      "Processed 42000/52359 comments (20.92 comments/sec, estimated total time: 41.7 min)\n",
      "Processed 42500/52359 comments (21.04 comments/sec, estimated total time: 41.5 min)\n",
      "Processed 43000/52359 comments (21.16 comments/sec, estimated total time: 41.2 min)\n",
      "Processed 43500/52359 comments (21.28 comments/sec, estimated total time: 41.0 min)\n",
      "Processed 44000/52359 comments (21.39 comments/sec, estimated total time: 40.8 min)\n",
      "Processed 44500/52359 comments (21.50 comments/sec, estimated total time: 40.6 min)\n",
      "Processed 45000/52359 comments (21.62 comments/sec, estimated total time: 40.4 min)\n",
      "Processed 45500/52359 comments (21.73 comments/sec, estimated total time: 40.2 min)\n",
      "Processed 46000/52359 comments (21.84 comments/sec, estimated total time: 40.0 min)\n",
      "Processed 46500/52359 comments (21.94 comments/sec, estimated total time: 39.8 min)\n",
      "Processed 47000/52359 comments (22.05 comments/sec, estimated total time: 39.6 min)\n",
      "Processed 47500/52359 comments (22.16 comments/sec, estimated total time: 39.4 min)\n",
      "Processed 48000/52359 comments (22.26 comments/sec, estimated total time: 39.2 min)\n",
      "Processed 48500/52359 comments (22.36 comments/sec, estimated total time: 39.0 min)\n",
      "Processed 49000/52359 comments (22.46 comments/sec, estimated total time: 38.8 min)\n",
      "Processed 49500/52359 comments (22.57 comments/sec, estimated total time: 38.7 min)\n",
      "Processed 50000/52359 comments (22.66 comments/sec, estimated total time: 38.5 min)\n",
      "Processed 50500/52359 comments (22.76 comments/sec, estimated total time: 38.3 min)\n",
      "Processed 51000/52359 comments (22.86 comments/sec, estimated total time: 38.2 min)\n",
      "Processed 51500/52359 comments (22.95 comments/sec, estimated total time: 38.0 min)\n",
      "Processed 52000/52359 comments (23.05 comments/sec, estimated total time: 37.9 min)\n",
      "Analysis completed in 2264.88 seconds (23.11 comments/sec)\n",
      "Results saved to loltyler_with_toxicity.csv\n",
      "\n",
      "===== TOXICITY DISTRIBUTION =====\n",
      "Total comments analyzed: 52352\n",
      "\n",
      "Distribution by toxicity type:\n",
      "Non-toxic: 47902.0 comments (91.5%)\n",
      "General Toxicity: 2533.0 comments (4.84%)\n",
      "Obscene: 1619.0 comments (3.09%)\n",
      "Insult: 214.0 comments (0.41%)\n",
      "Threat: 46.0 comments (0.09%)\n",
      "Identity Attack: 38.0 comments (0.07%)\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"loltyler.csv\"\n",
    "    \n",
    "results = analyze_toxicity(csv_file_path, save_results=True, sample_size=None)\n",
    "\n",
    "toxicity_distribution = get_toxicity_distribution(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
